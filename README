##############################################################################################
# Code for experiments in "Second Order PAC-Bayesian Bounds for the Weighted Majority Vote." #
##############################################################################################

Paper: Second Order PAC-Bayesian Bounds for the Weighted Majority Vote.
NeurIPS submission: 5551




*** Basic info and dependencies ***
The implementation is in Python 3. The implementation and provided scripts are tested under
Arch Linux, but should run in any Linux distro (and the python code should run in Windows/MAC).
The implementation uses the following standard python libraries:
- numpy
- scipy
- sklearn [2]

Furthermore, CMA [6] can be used for optimization of the bounds, if available.




*** Directory structure ***
The repository has the following directory structure (important files and directories shown):
README                               : this file
mvb/mvbase.py                        : framework for majority vote classifiers with bounds
mvb/rfc.py                           : implementation/wrapper of random forest
mvb/bounds/                          : sub-module containing code for computing and optimizing bounds
mvb/bounds/{c.py,pbkl.py,tools.py}   : implementation based on code from [1]
experiments/                         : directory containing code for experiments
experiments/uniform.py               : file for running experiments with uniform weighting
experiments/optimize.py              : file for running experiments with optimized weighting
experiments/unlabeled.py             : file for running experiments with unlabeled data
experiments/Makefile                 : Makefile for running full experimental setup in the paper




*** Example basic usage ***
First, setup the experiments directory by cd'ing to the directory and running 'make'. This downloads
data sets from [3,4,5] and creates a link to mvb. 

Then, to train and evaluate a random forest with uniform weights (using 20% of data for testing), use: 

	python uniform.py <data_set> [M] [sample_mode] [repeats]

where sample_mode denotes the bagging mode and must be 'bootstrap' (=full bagging), 'dim' (=sample
dim points for each tree) or a float 0<f<=1 (=sample f points for each tree). This will create the
file 'out/uniform/<data_set>-<M>-<sample_mode>.csv' containing <repeats> lines with various stats
and bounds (-1 if bound is not applicable).

E.g. to train a random forest for Cod-RNA with M=100 trees, using bagging and repeat the experiment
50 times, use:

	python uniform.py Cod-RNA 100 bootstrap 50

or if using reduced bagging:

	python uniform.py Cod-RNA 100 0.5 50

In a similar fashion, to train and evaluate a random forest with optimized weights, use:

	python optimize.py <data_set> [M] [sample_mode] [optimizer] [repeats]

where <optimizer> in {CMA, GD, RProp, iRProp}. This also creates the file rho-<data_set>.csv
containing the <M> rows with the resulting distributions.

To run an experiment with unlabeled data, use:

	python unlabeled.py <data_set> [M] [r] [repeats]

where <r> is a float in [0,1], denoting the amount of labeled training data; the remaining
(1-r) data is used unlabeled for computing the DIS bound.




*** Replicating experiments ***
A Makefile 'experiments/Makefile' is supplied for running all experiments from the paper.

1) cd to the directory 'experiments' and run 'make' to download data and create a link to mvb.
2) Run the relevant experiment by using the corresponding make target:
   - 'make uniform':          uniformly weighted random forest (Figure 1/Section H.2)
   - 'make optimize':         random forest with optimized weights (Figure 2/Section H.3)
   - 'make uniform-reduced':  uniformly weighted random forest with reduced bagging (Section H.4)
   - 'make optimize-reduced': random forest with optimized weights and reduced bagging (Section H.4)
   - 'make unlabeled':        random forest with varying amounts of unlabeled data (Section H.5)   
   Results are saved in out/{uniform,optimize,unlabeled} in separate files containing 50 repeats
   each (20 for unlabeled). Computing means/std. deviations/medians/quantiles and combining will
   produce the reuslts in the paper.

Note that a seed is set in the top of each file 'experiments/{uniform,optimize,unlabeled}.py'
for reproducibility.




*** References ***
[1] Germain, Lacasse, Laviolette, Marchand and Roy: Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm (JMLR 2015)
[2] The sklearn.ensemble module (https://scikit-learn.org/stable/modules/ensemble.html)
[3] The UCI Repository (https://archive.ics.uci.edu/ml/index.php)
[4] LibSVM (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)
[5] Zalando Research (https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/)
[6] CMA (https://pypi.org/project/cma/)
